# Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions

```
@article{oviatt2000designing,
  title={Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions},
  author={Oviatt, Sharon and Cohen, Phil and Wu, Lizhong and Duncan, Lisbeth and Suhm, Bernhard and Bers, Josh and Holzman, Thomas and Winograd, Terry and Landay, James and Larson, Jim and others},
  journal={Human-computer interaction},
  volume={15},
  number={4},
  pages={263--322},
  year={2000},
  publisher={Taylor \& Francis}
}
```
## One Sentence
...

> "... we summarize the emerging architectural approaches for interpreting speech and pen-based gestural input in a robust manner-including early and late fusion approaches, and the new hybrid symbolic-statistical approach."

## Key Points
> "To date, multimodal systems that combine either speech and pen input ... or speech and lip movements ... consititute two major research areas within the field."

#### Where speech is beneficial/useful?
> "Users tend to prefer speech for functions like describing objects and events, sets and subsets of objects, out-of-view objects, conjoined information, and past and future temporal states, as well as for issuing commands for actions or iterative actions. ... During multimodal pen-voice interaction, users tend to prefer entering descriptive information via speech, although their preference for pen input increases for digits, symbols, and graphic content ..."
> "For example analysis of the linguistic content of users' integrated pen-voice constructions has revealed that basic subject, verb, and object constituents almost always are spoken, whereas those describing locative information invariably are written or gestured."

#### Five ways a multimodal approach facilitates error recovery
1. Users will select the mode they judge to be less error-prone ("for particular lexical content");
2. Users' language is simplified when interacting multimodally, makinig it easier for NLP and reducing errors;
3. "Third, users have a strong tendency to switch modes after system errors, which facilitates error recovery.";
4. Users report less subjective frsutration when interacting multimodally;
5. Well architectured, a system can support mutual-disambiguation ("Mutual disambiguation involve recovery from unimodal recognition errors within a multimodal architecture because semantic information from each input mode supplies partial disambiguation of the other mode, thereby leading to more stable and robust overall system performance."

Benefits of speech + pen:
* Permit flexible use of input modes, including alternation and integrated use.
* Support improved efficiency, especially when manipulating graphical
information.
* Can support less disfluent, shorter, and more linguistically simplified
constructions than a speech-only interface, which results in more robust
NLP.
* Support greater precision of spatial information than a speech-only interface,
because pen input conveys rich and precise graphical information.
* Satisfy higher levels of user preference.
* Support enhanced error avoidance and ease of error resolution.
