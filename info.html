
<body>
    <link rel="stylesheet" href="style.css"/>
	<link href="https://fonts.googleapis.com/css?family=Alegreya+Sans:400,700" rel="stylesheet">

<p>
<a href="image/chen_photo.jpg">PHOTO</a>
</p>

<h3>TITLE </h3>
Cross-Device Interaction by Sensor Correlation


<h3>ABSTRACT </h3>
	Although users today frequently interact with a diverse set of devices, many software systems are still programmed to run mostly on single devices alone.
	What if we change this fundamental assumption, where instead systems will by default run on multiple devices? How will this change the way we design interactive systems? What new interactive experiences can we provide to the users?
	<br/>
	My research goal is to explore these questions by building interactive systems that span and leverage a combination of multiple networked devices.
	To achieve this goal, I have been taking a sensor correlation approach to enable novel user experience of cross-device interaction. 
	Specifically, by correlating sensors across devices, we can capture rich information about usersâ€™ behavior, intent and context, which was not accessible using each devices alone. 
	I will describe a framework for design-time sensor correlation, as demonstrated in three systems that span wearable, mobile, to room-scale devices.
	I will also discuss a run-time sensor correlation approach, which allows users to customize their own cross-device interaction via simple programming by demonstration.
<!-- We are looking towards a future where a wide range of fabrication machines--from desktop 3D printers to industrial robotic arms--will allow end-users to create functional objects of their designs. For end-users, traditional Computer-Aided Design tools are notoriously difficult to use, as they require complex and tedious tasks of manipulating low-level geometry. -->

<!-- In my research, I develop computational tools that enable end-users to express and iterate high-level ideas into custom designs. For example, Forte lets users sketch a design and annotate its functional descriptions, which then generates sturctures that continuously address users's evolving design intent while ensuring structural integrity; Reprise allows users to specify how they want to adapt existing objects for accessibility, and generates add-on structures to mechanically modify the usage of these objects; Medley provides a library of embeddables--everyday objects that be cut, worked and embedded into users' customized design to explore rich reusable material properties. I will describe an achitectural framework underpinning all these tools, where the ultimate goal is three-fold: providing users with domain-specific knowledge, expressive interaction techniques, and computational support. -->

<h3>BIO</h3>
Xiang 'Anthony' Chen is a Research Scientist at Tableau Research, focusing on enabling users to interact with data on various digital platforms. 
	He recently graduated from his PhD working with Scott Hudson and Stelian Coros in the School of Computer Science, Carnegie Mellon University. 
	His research develops technical and design approaches to build novel human-computer interfaces that enhance users' physical interactivity with ubiquitous computers, or enable their creativity in fabricating physical objects of their design (e.g., using 3D printing). 
	Anthony is an Adobe Research Fellow in Human-Computer Interaction. 
	Frequently collaborating with industrial research labs (Microsoft, Autodesk, and Google), he has published 17 papers in top-tier HCI conferences and journal (CHI, UIST, and TOCHI) and has received two best paper awards.

</body>
