<body>
	<link rel="stylesheet" href="style.css" />
	<link href="https://fonts.googleapis.com/css?family=Alegreya+Sans:400,700" rel="stylesheet">

	<table width="50%">
		<tr>
			<td>
				<a href="image/xac18.jpg" target="_blank">PHOTO</a>
				<br /><br />
			</td>
		</tr>

		<tr>
			<td>
				<b>TITLE </b>
				<br />
				Expanding the Interaction Bandwidth between Human and AI
				<br /><br />
			</td>
		</tr>

		<tr>

			<td>
				<b>ABSTRACT </b>
				<br />
				The recent development of artificial intelligence (AI) promises a future of data-driven automation that
				can replace most of today’s human efforts. However, currently most AI-enabled systems—often functioning
				as ‘black boxes’—struggle to accommodate, learn from or communicate with humans. One fundamental problem
				is a limited interaction bandwidth between human and AI: currently, AI’s development is bestowed upon
				the few experts; for users in non-computing domains, there is limited support for them to comprehend,
				customize or collaborate with AI. As we are on the cusp of defining the future of human-AI relationship,
				it is important to create new interaction channels to bridge AI and non-computing users. In this talk, I
				will discuss three research thrusts for expanding the interaction bandwidth between human and AI:
				<ul>
					<li>
						Human ← AI: making AI comprehensible to non-computing domain users. Going beyond system-centered
						prior work that focused on generic explainable representations of AI (XAI), my research takes a
						user-centered approach: for example, CheXplain is a system co-designed via iterative studies
						with medical professionals, which enables referring physicians to explore and understand AI’s
						diagnosis on chest X-ray images.
					</li>
					<li>
						Human → AI: enabling non-computing domain users to customize AI. Contrary to providing a single
						label and relying on AI to reverse-engineer the reasoning process, my research investigates
						techniques for users to express their domain knowledge in ways that are understandable and
						learnable to an AI: for example, Robiot employs computer vision to interpret a user’s
						demonstration of a physical task, which is translated in a vocabulary that informs an AI to
						generate robotic mechanisms to automate such tasks.

					</li>
					<li>
						Human ↔ AI: supporting collaboration between non-computing domain users and AI. Building upon
						comprehensible and customizable AI, my research takes an integrated approach that creates an
						environment to support human-AI collaboration: for example, Forte allows a mechanical engineer
						to sketch a high-level functional and aesthetic design while an AI handles the low-level
						generation of structures that realizes such design.

					</li>
				</ul>

			</td>
		</tr>

		<tr>
			<td>
				<b>BIO</b>
				<br />
				Xiang ‘Anthony' Chen is an Assistant Professor in UCLA's Department of Electrical & Computer
				Engineering. Anthony's area of expertise is Human-Computer Interaction (HCI). He received his Ph.D. in
				the School of Computer Science at Carnegie Mellon University in 2017 and was a recipient of the NSF CISE
				CRII Award and the Adobe Ph.D. Fellowship. His research is at the intersection of sensing & interaction
				techniques, intelligent user interfaces, and computational design & fabrication. Anthony’s work has won
				two best paper awards and one honorable mentioned in top-tier HCI conferences.
			</td>
		</tr>

	</table>


</body>